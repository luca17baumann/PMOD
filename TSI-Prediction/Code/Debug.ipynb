{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read combine debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function taken from utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    ''' Expected structure of the file: \n",
    "        one column name per line (no header)\n",
    "    '''\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            column_names = [line.strip() for line in file]\n",
    "            return column_names\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed with the read combine file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy\n",
    "from astropy.io import fits\n",
    "from astropy.time import Time\n",
    "from astropy import units as u\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_L1_PATH = '/Users/luca/Desktop/Internship/PMOD/TSI-Prediction/Data/Level1A'\n",
    "SOURCE_L2_PATH = '/Users/luca/Desktop/Internship/PMOD/TSI-Prediction/Data/Level2A'\n",
    "\n",
    "TARGET_PATH = \"/Users/luca/Desktop/Internship/PMOD/TSI-Prediction/Data/combined_data.pkl\"\n",
    "\n",
    "time = \"Irradiance TimeJD\"\n",
    "target = \"irradiance_B [W.m-2]\"\n",
    "target2 = \"irradiance_A [W.m-2]\"\n",
    "target3 = \"irradiance_C [W.m-2]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_subfolders(folder_path):\n",
    "    ''' Expected folder structure:\n",
    "    \n",
    "        folder_path/\n",
    "        ├── month1/                  \n",
    "        │   └── day1-file\n",
    "        │   └── day2-file    \n",
    "        │   └── ...\n",
    "        ├── month2/ \n",
    "        ... \n",
    "    '''\n",
    "    file_paths = []\n",
    "    for month in os.listdir(folder_path):\n",
    "        try:\n",
    "            if(month != \".DS_Store\"):\n",
    "                print(month)\n",
    "                for day in os.listdir(folder_path + \"/\" + month):\n",
    "                    if(day != \".DS_Store\"):\n",
    "                        print(day)\n",
    "                        file_paths.append(folder_path + \"/\" + month + \"/\" + day)\n",
    "        except:\n",
    "            print(\"No files in this folder\")\n",
    "    print(len(file_paths))\n",
    "    return file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_level1(path: str, features_level_1: list) -> pd.DataFrame:\n",
    "    \"\"\"Read as file from level1 and returns a dataframe indexed by time\n",
    "    \n",
    "    Args:\n",
    "        path (str) -> where to read the files from\n",
    "        features_level_1 (list) -> a list of the desired features\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    all_data = fits.open(path)\n",
    "\n",
    "    # only keep time until minute\n",
    "    TimeJD = all_data[8].data.field(\"Timestamp\")\n",
    "    TimeJD = Time(TimeJD, format=\"isot\")\n",
    "    TimeJD = TimeJD.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    data = {\"TimeJD\": TimeJD}\n",
    "    \n",
    "    try: \n",
    "        for feature in features_level_1:\n",
    "            data[feature] = all_data[8].data.field(feature)\n",
    "        dtypes = {feature: float for feature in features_level_1}\n",
    "        data = pd.DataFrame(data).astype(dtypes)\n",
    "\n",
    "        # groupby TimeJD and take the mean (there are multiple measurements per minute)\n",
    "        data[\"TimeJD\"] = pd.to_datetime(data[\"TimeJD\"])\n",
    "        data_per_minute = data.groupby(\"TimeJD\").mean()\n",
    "        data_per_minute.reset_index(inplace=True)\n",
    "\n",
    "        return data_per_minute\n",
    "    \n",
    "    except:\n",
    "        print(f'File {path} did not contained (some of) the columns specified and was excluded.')\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_level1(paths: list, features_level_1: list) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates all files from level1 and returns a dataframe indexed by time\n",
    "\n",
    "    Args:\n",
    "        paths (list)\n",
    "        features_level_1 (list) -> a list of the desired features\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for path in paths:\n",
    "        next_day = read_file_level1(path, features_level_1)\n",
    "        df = pd.concat([df, next_day])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_level2(path: str, time:str, target:str, target2: str = None, target3: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Reads file from level2 and returns a dataframe indexed by time\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the file\n",
    "        time (str): Name of the time field\n",
    "        target (str): Name of the target field\n",
    "        target2 (str, optional): Name of the second target field (default: None)\n",
    "        target3 (str, optional): Name of the third target field (default: None)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    all_data = fits.open(path)\n",
    "    TimeJD = all_data[1].data.field(time)  # time in Julian day\n",
    "    TimeJD = Time(TimeJD, format=\"jd\", scale=\"utc\").iso\n",
    "    \n",
    "    IrrB = all_data[1].data.field(target)  # estimated TSI for cavity B\n",
    "    \n",
    "    if target2 is not None:\n",
    "        IrrA = all_data[1].data.field(target2)  # estimated TSI for cavity A\n",
    "    else:\n",
    "        IrrA = None\n",
    "    \n",
    "    if target3 is not None:\n",
    "        IrrC = all_data[1].data.field(target3)  # estimated TSI for cavity C\n",
    "    else:\n",
    "        IrrC = None\n",
    "    \n",
    "\n",
    "    data = {\"TimeJD\": TimeJD, \"IrrA\": IrrA, \"IrrB\": IrrB, \"IrrC\": IrrC}\n",
    "\n",
    "    dtypes = {\n",
    "        \"IrrA\": float,\n",
    "        \"IrrB\": float,\n",
    "        \"IrrC\": float,\n",
    "    }\n",
    "    \n",
    "    data = pd.DataFrame(data).astype(dtypes)\n",
    "    \n",
    "    if target2 is None:\n",
    "        data.drop(columns=[\"IrrA\"], inplace=True)\n",
    "    if target3 is None:\n",
    "        data.drop(columns=[\"IrrC\"], inplace=True)\n",
    "\n",
    "    # groupby TimeJD and take the mean (there are multiple measurements per minute). Takes also care of the NaN values\n",
    "    data[\"TimeJD\"] = pd.to_datetime(data[\"TimeJD\"], errors=\"coerce\")\n",
    "    data.dropna(subset=[\"TimeJD\"], inplace=True)\n",
    "    data[\"TimeJD\"] = data[\"TimeJD\"].dt.floor(\"T\")\n",
    "    data_per_minute = data.groupby(\"TimeJD\").mean()\n",
    "    data_per_minute.reset_index(inplace=True)\n",
    "\n",
    "    return data_per_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_level2(paths: list, time:str, target:str, target2: str = None, target3: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates all files from level2 and returns a dataframe indexed by time\n",
    "\n",
    "    Args:\n",
    "        paths (list)\n",
    "        time (str): Name of the time field\n",
    "        target (str): Name of the target field\n",
    "        target2 (str, optional): Name of the second target field (default: None)\n",
    "        target3 (str, optional): Name of the third target field (default: None)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for path in paths:\n",
    "        next_day = read_file_level2(path, time, target, target2, target3)\n",
    "        df = pd.concat([df, next_day])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_level1_level2(df_level1: pd.DataFrame, df_level2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merges dataframes from level1 and level2 and returns a dataframe indexed by time\n",
    "\n",
    "    Args:\n",
    "        df_level1 (pd.DataFrame)\n",
    "        df_level2 (pd.DataFrame)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    df = df_level1.merge(df_level2, on=\"TimeJD\", how = \"left\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09\n",
      "2021-09-02_L1a.fits\n",
      "2021-09-01_L1a.fits\n",
      "2\n",
      "09\n",
      "Lv_2A_2021-09-01_L1a.fits_created-2023-10-18_16-35-05.fits\n",
      "Lv_2A_2021-09-02_L1a.fits_created-2023-10-18_16-36-56.fits\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludovicacattaneo/anaconda3/envs/dsl/lib/python3.10/site-packages/erfa/core.py:154: ErfaWarning: ERFA function \"d2dtf\" yielded 1675762 of \"dubious year (Note 5)\"\n",
      "  warnings.warn('ERFA function \"{}\" yielded {}'.format(func_name, wmsg),\n",
      "/Users/ludovicacattaneo/anaconda3/envs/dsl/lib/python3.10/site-packages/erfa/core.py:154: ErfaWarning: ERFA function \"d2dtf\" yielded 1680144 of \"dubious year (Note 5)\"\n",
      "  warnings.warn('ERFA function \"{}\" yielded {}'.format(func_name, wmsg),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: complete. Data file in  /Users/ludovicacattaneo/Desktop/TSI-Prediction/Data/combined_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# LEFT JOIN (including gap data)\n",
    "    \n",
    "features_level_1 = ['CAV_HEATSINK_TEMP', 'CAV_HS_HEATER_VOLTAGE']\n",
    "\n",
    "paths_files_level1 = list_files_in_subfolders(SOURCE_L1_PATH)\n",
    "paths_files_level2 = list_files_in_subfolders(SOURCE_L2_PATH)\n",
    "\n",
    "# Read all files\n",
    "df_level1 = concatenate_level1(paths_files_level1, features_level_1)\n",
    "df_level2 = concatenate_level2(paths_files_level2, time, target, target2, target3)\n",
    "\n",
    "# Combine the data from level1 and level2 and save to pickle file\n",
    "df_combined = merge_level1_level2(df_level1, df_level2)\n",
    "df_combined.to_pickle(TARGET_PATH)\n",
    "\n",
    "print(\"Merging: complete. Data file in \", TARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
